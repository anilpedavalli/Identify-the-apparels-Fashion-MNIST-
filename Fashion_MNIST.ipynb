{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Fashion MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "p_-09sNi53Uc",
        "outputId": "42e74560-2f1b-43ca-bd32-70fea200fb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: datahack-prod.s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7,te;q=0.6\" --header=\"Referer: https://datahack.analyticsvidhya.com/\" \"https://datahack-prod.s3.amazonaws.com/train_zip/train_LbELtWX.zip\" -c -O 'train_LbELtWX.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-14 12:45:07--  https://datahack-prod.s3.amazonaws.com/train_zip/train_LbELtWX.zip\n",
            "Resolving datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)... 52.219.62.8\n",
            "Connecting to datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)|52.219.62.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82498602 (79M) [application/zip]\n",
            "Saving to: ‘train_LbELtWX.zip’\n",
            "\n",
            "train_LbELtWX.zip   100%[===================>]  78.68M  21.7MB/s    in 3.6s    \n",
            "\n",
            "2020-10-14 12:45:11 (21.7 MB/s) - ‘train_LbELtWX.zip’ saved [82498602/82498602]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lcOK6VDy53U-",
        "outputId": "fbcb5c16-0061-494c-a7fc-b200e217766c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: datahack-prod.s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7,te;q=0.6\" --header=\"Referer: https://datahack.analyticsvidhya.com/\" \"https://datahack-prod.s3.amazonaws.com/test_zip/test_ScVgIM0.zip\" -c -O 'test_ScVgIM0.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-14 12:45:11--  https://datahack-prod.s3.amazonaws.com/test_zip/test_ScVgIM0.zip\n",
            "Resolving datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)... 52.219.64.104\n",
            "Connecting to datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)|52.219.64.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13781763 (13M) [application/zip]\n",
            "Saving to: ‘test_ScVgIM0.zip’\n",
            "\n",
            "test_ScVgIM0.zip    100%[===================>]  13.14M  10.9MB/s    in 1.2s    \n",
            "\n",
            "2020-10-14 12:45:13 (10.9 MB/s) - ‘test_ScVgIM0.zip’ saved [13781763/13781763]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-IDQP-_c53VR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "R2l9EZLd53Vj"
      },
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"train_LbELtWX.zip\")\n",
        "shutil.unpack_archive(\"test_ScVgIM0.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bCfjHkyu53V3",
        "outputId": "0d77dd4e-074b-427d-d477-e53a80cd461f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  test.csv          \u001b[01;34mtrain\u001b[0m/     train_LbELtWX.zip\n",
            "\u001b[01;34mtest\u001b[0m/         test_ScVgIM0.zip  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U8tGF8S453WL",
        "outputId": "e1982b53-90e0-4ce6-82c4-f036ca240bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "train_df=pd.read_csv('train.csv')\n",
        "train_df['id']=train_df['id'].apply(lambda x:str(x)+'.png')\n",
        "train_df['label']=train_df['label'].apply(lambda x:str(x))\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.png</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.png</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id label\n",
              "0  1.png     9\n",
              "1  2.png     0\n",
              "2  3.png     0\n",
              "3  4.png     3\n",
              "4  5.png     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9swwP2a6Jx6",
        "outputId": "04800aab-83cb-47d2-b95b-4e02d45b5c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ooAomGKJ53Wb",
        "outputId": "19749c2f-d9b3-47e1-d337-690621ac4e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "img=image.load_img('/content/train/1.png',target_size=(224,224))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F09FA664AC8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fyS7sex253Wx",
        "outputId": "8b80561f-e1cc-41e5-bd5e-8a519f12a864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "img=image.load_img('/content/test/test_images/60007.png',target_size=(224,224))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F16612409E8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z47KMA83L_NJ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1/255., \n",
        "shear_range = 0.2,\n",
        "rotation_range=20,\n",
        "horizontal_flip=True,\n",
        "width_shift_range=0.15,\n",
        "height_shift_range=0.15,\n",
        "zoom_range=0.2,\n",
        "validation_split=0.2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VKYjSK4I53XD",
        "outputId": "558d78d4-fd3c-47ea-921f-f7862b82ca6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1/255.,\n",
        "rotation_range=25,\n",
        "horizontal_flip=True,\n",
        "width_shift_range=0.15,\n",
        "height_shift_range=0.15,\n",
        "zoom_range=0.15,                               \n",
        "validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(dataframe=train_df, \n",
        "directory='/content/train',\n",
        "x_col='id',\n",
        "y_col='label',\n",
        "target_size=(224,224),\n",
        "class_mode='categorical',\n",
        "batch_size=32,\n",
        "subset='training',\n",
        "seed=7)\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(dataframe=train_df, \n",
        "directory='/content//train',\n",
        "x_col='id',\n",
        "y_col='label',\n",
        "target_size=(224,224),\n",
        "class_mode='categorical',\n",
        "batch_size=32,\n",
        "subset='validation',\n",
        "seed=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 48000 validated image filenames belonging to 10 classes.\n",
            "Found 12000 validated image filenames belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PFkrB0-C53XR"
      },
      "source": [
        "#Callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "filepath_1=\"best_model_1.hdf5\"\n",
        "filepath_2=\"best_model_2.hdf5\"\n",
        "filepath_3=\"best_model_3.hdf5\"\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "checkpoint_3 = ModelCheckpoint(filepath_3, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "es = EarlyStopping(monitor='val_acc', mode='max',patience=25)\n",
        "\n",
        "\n",
        "callbacks_list_1 = [checkpoint_1,es]\n",
        "callbacks_list_2 = [checkpoint_2,es]\n",
        "callbacks_list_3 = [checkpoint_3,es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w4B1SAfw53Xl"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D ,GlobalAveragePooling2D,Flatten\n",
        "from tensorflow.keras.layers import Activation,MaxPooling2D,Dropout,BatchNormalization\n",
        "\n",
        "base_model=VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "model=Flatten()(base_model.layers[-1].output)\n",
        "#model=Dense(512,activation='relu')(model)\n",
        "#model=Dropout(0.2)(model)\n",
        "model=Dense(256,activation='relu')(model)\n",
        "model=Dense(128,activation='relu')(model)\n",
        "model=Dropout(0.2)(model)\n",
        "model=Dense(32,activation='relu')(model)\n",
        "model=Dense(16,activation='relu')(model)\n",
        "model_output=Dense(10,activation='softmax')(model)\n",
        "\n",
        "model_1=Model(inputs=base_model.input,outputs=model_output)\n",
        "\n",
        "#model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avER1SdPziB7",
        "outputId": "1c3e68f0-ed8a-4f9b-cbe2-e11adcbc9642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_1.compile(optimizer='adam',metrics='acc',loss='categorical_crossentropy')\n",
        "hist=model_1.fit_generator(train_generator,epochs=350,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n",
        "  validation_steps=len(validation_generator)//32,callbacks=[callbacks_list_1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            " 2/46 [>.............................] - ETA: 4s - loss: 2.7070 - acc: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0655s vs `on_train_batch_end` time: 0.1384s). Check your callbacks.\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.2922 - acc: 0.1372\n",
            "Epoch 00001: val_acc improved from 0.11364 to 0.32386, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 236ms/step - loss: 2.2922 - acc: 0.1372 - val_loss: 2.1164 - val_acc: 0.3239\n",
            "Epoch 2/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.9133 - acc: 0.2935\n",
            "Epoch 00002: val_acc improved from 0.32386 to 0.51136, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 230ms/step - loss: 1.9133 - acc: 0.2935 - val_loss: 1.4629 - val_acc: 0.5114\n",
            "Epoch 3/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.4567 - acc: 0.4939\n",
            "Epoch 00003: val_acc improved from 0.51136 to 0.61364, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 1.4567 - acc: 0.4939 - val_loss: 1.1609 - val_acc: 0.6136\n",
            "Epoch 4/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0804 - acc: 0.6332\n",
            "Epoch 00004: val_acc improved from 0.61364 to 0.78693, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 232ms/step - loss: 1.0804 - acc: 0.6332 - val_loss: 0.7339 - val_acc: 0.7869\n",
            "Epoch 5/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8434 - acc: 0.7092\n",
            "Epoch 00005: val_acc did not improve from 0.78693\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.8434 - acc: 0.7092 - val_loss: 0.6951 - val_acc: 0.7102\n",
            "Epoch 6/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7317 - acc: 0.7283\n",
            "Epoch 00006: val_acc did not improve from 0.78693\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.7317 - acc: 0.7283 - val_loss: 0.6326 - val_acc: 0.7699\n",
            "Epoch 7/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7174 - acc: 0.7323\n",
            "Epoch 00007: val_acc improved from 0.78693 to 0.80682, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.7174 - acc: 0.7323 - val_loss: 0.5385 - val_acc: 0.8068\n",
            "Epoch 8/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6342 - acc: 0.7819\n",
            "Epoch 00008: val_acc did not improve from 0.80682\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.6342 - acc: 0.7819 - val_loss: 0.5904 - val_acc: 0.7841\n",
            "Epoch 9/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5796 - acc: 0.7914\n",
            "Epoch 00009: val_acc did not improve from 0.80682\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.5796 - acc: 0.7914 - val_loss: 0.5734 - val_acc: 0.7926\n",
            "Epoch 10/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5688 - acc: 0.8003\n",
            "Epoch 00010: val_acc did not improve from 0.80682\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.5688 - acc: 0.8003 - val_loss: 0.5343 - val_acc: 0.8040\n",
            "Epoch 11/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5505 - acc: 0.8071\n",
            "Epoch 00011: val_acc did not improve from 0.80682\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.5505 - acc: 0.8071 - val_loss: 0.5757 - val_acc: 0.7926\n",
            "Epoch 12/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5706 - acc: 0.7928\n",
            "Epoch 00012: val_acc did not improve from 0.80682\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.5706 - acc: 0.7928 - val_loss: 0.5944 - val_acc: 0.7869\n",
            "Epoch 13/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5005 - acc: 0.8179\n",
            "Epoch 00013: val_acc improved from 0.80682 to 0.81250, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 234ms/step - loss: 0.5005 - acc: 0.8179 - val_loss: 0.4639 - val_acc: 0.8125\n",
            "Epoch 14/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5040 - acc: 0.8193\n",
            "Epoch 00014: val_acc improved from 0.81250 to 0.81818, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.5040 - acc: 0.8193 - val_loss: 0.4616 - val_acc: 0.8182\n",
            "Epoch 15/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5118 - acc: 0.8159\n",
            "Epoch 00015: val_acc improved from 0.81818 to 0.86080, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 237ms/step - loss: 0.5118 - acc: 0.8159 - val_loss: 0.3799 - val_acc: 0.8608\n",
            "Epoch 16/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5338 - acc: 0.8057\n",
            "Epoch 00016: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.5338 - acc: 0.8057 - val_loss: 0.4471 - val_acc: 0.8295\n",
            "Epoch 17/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4571 - acc: 0.8376\n",
            "Epoch 00017: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.4571 - acc: 0.8376 - val_loss: 0.4065 - val_acc: 0.8580\n",
            "Epoch 18/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4548 - acc: 0.8390\n",
            "Epoch 00018: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 217ms/step - loss: 0.4548 - acc: 0.8390 - val_loss: 0.3752 - val_acc: 0.8580\n",
            "Epoch 19/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4585 - acc: 0.8383\n",
            "Epoch 00019: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.4585 - acc: 0.8383 - val_loss: 0.4589 - val_acc: 0.8295\n",
            "Epoch 20/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4621 - acc: 0.8342\n",
            "Epoch 00020: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.4621 - acc: 0.8342 - val_loss: 0.3864 - val_acc: 0.8494\n",
            "Epoch 21/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4849 - acc: 0.8342\n",
            "Epoch 00021: val_acc did not improve from 0.86080\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.4849 - acc: 0.8342 - val_loss: 0.4525 - val_acc: 0.8352\n",
            "Epoch 22/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4507 - acc: 0.8424\n",
            "Epoch 00022: val_acc improved from 0.86080 to 0.86932, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.4507 - acc: 0.8424 - val_loss: 0.4024 - val_acc: 0.8693\n",
            "Epoch 23/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4122 - acc: 0.8519\n",
            "Epoch 00023: val_acc did not improve from 0.86932\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.4122 - acc: 0.8519 - val_loss: 0.4367 - val_acc: 0.8438\n",
            "Epoch 24/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4334 - acc: 0.8526\n",
            "Epoch 00024: val_acc did not improve from 0.86932\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.4334 - acc: 0.8526 - val_loss: 0.4626 - val_acc: 0.8494\n",
            "Epoch 25/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3836 - acc: 0.8682\n",
            "Epoch 00025: val_acc improved from 0.86932 to 0.87216, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 232ms/step - loss: 0.3836 - acc: 0.8682 - val_loss: 0.3611 - val_acc: 0.8722\n",
            "Epoch 26/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3748 - acc: 0.8655\n",
            "Epoch 00026: val_acc did not improve from 0.87216\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3748 - acc: 0.8655 - val_loss: 0.4371 - val_acc: 0.8182\n",
            "Epoch 27/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3858 - acc: 0.8621\n",
            "Epoch 00027: val_acc did not improve from 0.87216\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3858 - acc: 0.8621 - val_loss: 0.4132 - val_acc: 0.8551\n",
            "Epoch 28/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3799 - acc: 0.8702\n",
            "Epoch 00028: val_acc improved from 0.87216 to 0.88636, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.3799 - acc: 0.8702 - val_loss: 0.3269 - val_acc: 0.8864\n",
            "Epoch 29/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3881 - acc: 0.8635\n",
            "Epoch 00029: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3881 - acc: 0.8635 - val_loss: 0.2924 - val_acc: 0.8835\n",
            "Epoch 30/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3713 - acc: 0.8635\n",
            "Epoch 00030: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3713 - acc: 0.8635 - val_loss: 0.3317 - val_acc: 0.8608\n",
            "Epoch 31/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3744 - acc: 0.8560\n",
            "Epoch 00031: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3744 - acc: 0.8560 - val_loss: 0.3489 - val_acc: 0.8778\n",
            "Epoch 32/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4191 - acc: 0.8533\n",
            "Epoch 00032: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 217ms/step - loss: 0.4191 - acc: 0.8533 - val_loss: 0.3159 - val_acc: 0.8835\n",
            "Epoch 33/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4660 - acc: 0.8458\n",
            "Epoch 00033: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.4660 - acc: 0.8458 - val_loss: 0.4758 - val_acc: 0.8324\n",
            "Epoch 34/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3999 - acc: 0.8648\n",
            "Epoch 00034: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3999 - acc: 0.8648 - val_loss: 0.4355 - val_acc: 0.8580\n",
            "Epoch 35/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3536 - acc: 0.8553\n",
            "Epoch 00035: val_acc did not improve from 0.88636\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3536 - acc: 0.8553 - val_loss: 0.3758 - val_acc: 0.8608\n",
            "Epoch 36/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3571 - acc: 0.8764\n",
            "Epoch 00036: val_acc improved from 0.88636 to 0.90057, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.3571 - acc: 0.8764 - val_loss: 0.2728 - val_acc: 0.9006\n",
            "Epoch 37/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3146 - acc: 0.8852\n",
            "Epoch 00037: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3146 - acc: 0.8852 - val_loss: 0.3563 - val_acc: 0.8722\n",
            "Epoch 38/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3660 - acc: 0.8723\n",
            "Epoch 00038: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3660 - acc: 0.8723 - val_loss: 0.2935 - val_acc: 0.8722\n",
            "Epoch 39/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3425 - acc: 0.8750\n",
            "Epoch 00039: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3425 - acc: 0.8750 - val_loss: 0.4149 - val_acc: 0.8494\n",
            "Epoch 40/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3307 - acc: 0.8818\n",
            "Epoch 00040: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3307 - acc: 0.8818 - val_loss: 0.3087 - val_acc: 0.8807\n",
            "Epoch 41/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3294 - acc: 0.8872\n",
            "Epoch 00041: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3294 - acc: 0.8872 - val_loss: 0.3257 - val_acc: 0.8949\n",
            "Epoch 42/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3645 - acc: 0.8668\n",
            "Epoch 00042: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3645 - acc: 0.8668 - val_loss: 0.3457 - val_acc: 0.9006\n",
            "Epoch 43/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3588 - acc: 0.8702\n",
            "Epoch 00043: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3588 - acc: 0.8702 - val_loss: 0.3041 - val_acc: 0.8920\n",
            "Epoch 44/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3387 - acc: 0.8662\n",
            "Epoch 00044: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3387 - acc: 0.8662 - val_loss: 0.3282 - val_acc: 0.8835\n",
            "Epoch 45/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3168 - acc: 0.8825\n",
            "Epoch 00045: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3168 - acc: 0.8825 - val_loss: 0.3745 - val_acc: 0.8835\n",
            "Epoch 46/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3603 - acc: 0.8743\n",
            "Epoch 00046: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3603 - acc: 0.8743 - val_loss: 0.3323 - val_acc: 0.8580\n",
            "Epoch 47/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3612 - acc: 0.8723\n",
            "Epoch 00047: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3612 - acc: 0.8723 - val_loss: 0.4092 - val_acc: 0.8835\n",
            "Epoch 48/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3547 - acc: 0.8723\n",
            "Epoch 00048: val_acc improved from 0.90057 to 0.90341, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 232ms/step - loss: 0.3547 - acc: 0.8723 - val_loss: 0.2949 - val_acc: 0.9034\n",
            "Epoch 49/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3213 - acc: 0.8845\n",
            "Epoch 00049: val_acc did not improve from 0.90341\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3213 - acc: 0.8845 - val_loss: 0.3589 - val_acc: 0.8750\n",
            "Epoch 50/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3814 - acc: 0.8573\n",
            "Epoch 00050: val_acc did not improve from 0.90341\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3814 - acc: 0.8573 - val_loss: 0.3224 - val_acc: 0.8977\n",
            "Epoch 51/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3494 - acc: 0.8804\n",
            "Epoch 00051: val_acc did not improve from 0.90341\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3494 - acc: 0.8804 - val_loss: 0.2882 - val_acc: 0.8949\n",
            "Epoch 52/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3401 - acc: 0.8764\n",
            "Epoch 00052: val_acc did not improve from 0.90341\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3401 - acc: 0.8764 - val_loss: 0.2777 - val_acc: 0.8892\n",
            "Epoch 53/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3062 - acc: 0.8940\n",
            "Epoch 00053: val_acc improved from 0.90341 to 0.92614, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 11s 231ms/step - loss: 0.3062 - acc: 0.8940 - val_loss: 0.2453 - val_acc: 0.9261\n",
            "Epoch 54/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3250 - acc: 0.8818\n",
            "Epoch 00054: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3250 - acc: 0.8818 - val_loss: 0.2411 - val_acc: 0.9261\n",
            "Epoch 55/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3105 - acc: 0.8893\n",
            "Epoch 00055: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3105 - acc: 0.8893 - val_loss: 0.3219 - val_acc: 0.8608\n",
            "Epoch 56/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3097 - acc: 0.8927\n",
            "Epoch 00056: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3097 - acc: 0.8927 - val_loss: 0.2364 - val_acc: 0.9233\n",
            "Epoch 57/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3229 - acc: 0.8764\n",
            "Epoch 00057: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3229 - acc: 0.8764 - val_loss: 0.2765 - val_acc: 0.8977\n",
            "Epoch 58/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2678 - acc: 0.8913\n",
            "Epoch 00058: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.2678 - acc: 0.8913 - val_loss: 0.3015 - val_acc: 0.8892\n",
            "Epoch 59/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3184 - acc: 0.8920\n",
            "Epoch 00059: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3184 - acc: 0.8920 - val_loss: 0.2829 - val_acc: 0.8892\n",
            "Epoch 60/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3284 - acc: 0.8825\n",
            "Epoch 00060: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3284 - acc: 0.8825 - val_loss: 0.2424 - val_acc: 0.9091\n",
            "Epoch 61/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3129 - acc: 0.8899\n",
            "Epoch 00061: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3129 - acc: 0.8899 - val_loss: 0.3354 - val_acc: 0.8835\n",
            "Epoch 62/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3309 - acc: 0.8920\n",
            "Epoch 00062: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3309 - acc: 0.8920 - val_loss: 0.3204 - val_acc: 0.8807\n",
            "Epoch 63/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3490 - acc: 0.8838\n",
            "Epoch 00063: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3490 - acc: 0.8838 - val_loss: 0.2344 - val_acc: 0.9034\n",
            "Epoch 64/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2984 - acc: 0.8947\n",
            "Epoch 00064: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.2984 - acc: 0.8947 - val_loss: 0.3516 - val_acc: 0.8551\n",
            "Epoch 65/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2745 - acc: 0.9035\n",
            "Epoch 00065: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.2745 - acc: 0.9035 - val_loss: 0.4047 - val_acc: 0.8580\n",
            "Epoch 66/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3346 - acc: 0.8784\n",
            "Epoch 00066: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3346 - acc: 0.8784 - val_loss: 0.2388 - val_acc: 0.9091\n",
            "Epoch 67/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2781 - acc: 0.9022\n",
            "Epoch 00067: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 220ms/step - loss: 0.2781 - acc: 0.9022 - val_loss: 0.3031 - val_acc: 0.8835\n",
            "Epoch 68/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3132 - acc: 0.8832\n",
            "Epoch 00068: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3132 - acc: 0.8832 - val_loss: 0.3250 - val_acc: 0.8920\n",
            "Epoch 69/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3164 - acc: 0.8940\n",
            "Epoch 00069: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3164 - acc: 0.8940 - val_loss: 0.3465 - val_acc: 0.8807\n",
            "Epoch 70/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3125 - acc: 0.8886\n",
            "Epoch 00070: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.3125 - acc: 0.8886 - val_loss: 0.2264 - val_acc: 0.9205\n",
            "Epoch 71/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2927 - acc: 0.8920\n",
            "Epoch 00071: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 220ms/step - loss: 0.2927 - acc: 0.8920 - val_loss: 0.2906 - val_acc: 0.8949\n",
            "Epoch 72/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.2834 - acc: 0.8961\n",
            "Epoch 00072: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.2834 - acc: 0.8961 - val_loss: 0.3074 - val_acc: 0.8864\n",
            "Epoch 73/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5145 - acc: 0.8743\n",
            "Epoch 00073: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.5145 - acc: 0.8743 - val_loss: 0.6165 - val_acc: 0.7926\n",
            "Epoch 74/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4692 - acc: 0.8458\n",
            "Epoch 00074: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.4692 - acc: 0.8458 - val_loss: 0.5898 - val_acc: 0.8381\n",
            "Epoch 75/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3301 - acc: 0.8804\n",
            "Epoch 00075: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3301 - acc: 0.8804 - val_loss: 0.3144 - val_acc: 0.8892\n",
            "Epoch 76/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3420 - acc: 0.8798\n",
            "Epoch 00076: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3420 - acc: 0.8798 - val_loss: 0.3101 - val_acc: 0.8807\n",
            "Epoch 77/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3544 - acc: 0.8784\n",
            "Epoch 00077: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3544 - acc: 0.8784 - val_loss: 0.3247 - val_acc: 0.8892\n",
            "Epoch 78/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3441 - acc: 0.8750\n",
            "Epoch 00078: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.3441 - acc: 0.8750 - val_loss: 0.3430 - val_acc: 0.8835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV6wbkaMj7R-",
        "outputId": "92d813ff-5b49-4dbd-a959-df8a7b5befa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_1.compile(optimizer='adam',metrics='acc',loss='categorical_crossentropy')\n",
        "hist=model_1.fit_generator(train_generator,epochs=350,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n",
        "  validation_steps=len(validation_generator)//32,callbacks=[callbacks_list_2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-374187935c72>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/350\n",
            " 2/46 [>.............................] - ETA: 4s - loss: 2.6866 - acc: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0676s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.3338 - acc: 0.1393\n",
            "Epoch 00001: val_acc improved from -inf to 0.22727, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 496ms/step - loss: 2.3338 - acc: 0.1393 - val_loss: 1.9715 - val_acc: 0.2273\n",
            "Epoch 2/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.9289 - acc: 0.2446\n",
            "Epoch 00002: val_acc improved from 0.22727 to 0.29261, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 496ms/step - loss: 1.9289 - acc: 0.2446 - val_loss: 1.7241 - val_acc: 0.2926\n",
            "Epoch 3/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.7507 - acc: 0.2806\n",
            "Epoch 00003: val_acc improved from 0.29261 to 0.34943, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 494ms/step - loss: 1.7507 - acc: 0.2806 - val_loss: 1.7687 - val_acc: 0.3494\n",
            "Epoch 4/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.6504 - acc: 0.3370\n",
            "Epoch 00004: val_acc improved from 0.34943 to 0.36080, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 494ms/step - loss: 1.6504 - acc: 0.3370 - val_loss: 1.4972 - val_acc: 0.3608\n",
            "Epoch 5/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.4163 - acc: 0.4205\n",
            "Epoch 00005: val_acc improved from 0.36080 to 0.44034, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 498ms/step - loss: 1.4163 - acc: 0.4205 - val_loss: 1.3604 - val_acc: 0.4403\n",
            "Epoch 6/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.3649 - acc: 0.4334\n",
            "Epoch 00006: val_acc did not improve from 0.44034\n",
            "46/46 [==============================] - 22s 474ms/step - loss: 1.3649 - acc: 0.4334 - val_loss: 1.3264 - val_acc: 0.3835\n",
            "Epoch 7/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.2769 - acc: 0.4776\n",
            "Epoch 00007: val_acc improved from 0.44034 to 0.49432, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 494ms/step - loss: 1.2769 - acc: 0.4776 - val_loss: 1.2233 - val_acc: 0.4943\n",
            "Epoch 8/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.2552 - acc: 0.5027\n",
            "Epoch 00008: val_acc improved from 0.49432 to 0.55114, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 494ms/step - loss: 1.2552 - acc: 0.5027 - val_loss: 1.1356 - val_acc: 0.5511\n",
            "Epoch 9/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1634 - acc: 0.5387\n",
            "Epoch 00009: val_acc did not improve from 0.55114\n",
            "46/46 [==============================] - 22s 480ms/step - loss: 1.1634 - acc: 0.5387 - val_loss: 1.1086 - val_acc: 0.5455\n",
            "Epoch 10/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1613 - acc: 0.5462\n",
            "Epoch 00010: val_acc did not improve from 0.55114\n",
            "46/46 [==============================] - 22s 476ms/step - loss: 1.1613 - acc: 0.5462 - val_loss: 1.1296 - val_acc: 0.5341\n",
            "Epoch 11/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0170 - acc: 0.5883\n",
            "Epoch 00011: val_acc improved from 0.55114 to 0.63068, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 495ms/step - loss: 1.0170 - acc: 0.5883 - val_loss: 1.0009 - val_acc: 0.6307\n",
            "Epoch 12/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0831 - acc: 0.5530\n",
            "Epoch 00012: val_acc did not improve from 0.63068\n",
            "46/46 [==============================] - 22s 475ms/step - loss: 1.0831 - acc: 0.5530 - val_loss: 1.1394 - val_acc: 0.5795\n",
            "Epoch 13/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0858 - acc: 0.5639\n",
            "Epoch 00013: val_acc did not improve from 0.63068\n",
            "46/46 [==============================] - 22s 473ms/step - loss: 1.0858 - acc: 0.5639 - val_loss: 0.9476 - val_acc: 0.6307\n",
            "Epoch 14/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9950 - acc: 0.6121\n",
            "Epoch 00014: val_acc improved from 0.63068 to 0.63636, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 496ms/step - loss: 0.9950 - acc: 0.6121 - val_loss: 0.9192 - val_acc: 0.6364\n",
            "Epoch 15/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9884 - acc: 0.6121\n",
            "Epoch 00015: val_acc did not improve from 0.63636\n",
            "46/46 [==============================] - 22s 476ms/step - loss: 0.9884 - acc: 0.6121 - val_loss: 0.9921 - val_acc: 0.5966\n",
            "Epoch 16/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9539 - acc: 0.6406\n",
            "Epoch 00016: val_acc improved from 0.63636 to 0.65057, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 495ms/step - loss: 0.9539 - acc: 0.6406 - val_loss: 0.9627 - val_acc: 0.6506\n",
            "Epoch 17/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9109 - acc: 0.6603\n",
            "Epoch 00017: val_acc did not improve from 0.65057\n",
            "46/46 [==============================] - 22s 481ms/step - loss: 0.9109 - acc: 0.6603 - val_loss: 0.8771 - val_acc: 0.6449\n",
            "Epoch 18/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8673 - acc: 0.6664\n",
            "Epoch 00018: val_acc improved from 0.65057 to 0.65625, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 497ms/step - loss: 0.8673 - acc: 0.6664 - val_loss: 0.9161 - val_acc: 0.6562\n",
            "Epoch 19/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8878 - acc: 0.6542\n",
            "Epoch 00019: val_acc did not improve from 0.65625\n",
            "46/46 [==============================] - 22s 476ms/step - loss: 0.8878 - acc: 0.6542 - val_loss: 0.8334 - val_acc: 0.6506\n",
            "Epoch 20/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8550 - acc: 0.6746\n",
            "Epoch 00020: val_acc improved from 0.65625 to 0.72443, saving model to best_model_2.hdf5\n",
            "46/46 [==============================] - 23s 494ms/step - loss: 0.8550 - acc: 0.6746 - val_loss: 0.7623 - val_acc: 0.7244\n",
            "Epoch 21/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8560 - acc: 0.6698\n",
            "Epoch 00021: val_acc did not improve from 0.72443\n",
            "46/46 [==============================] - 22s 479ms/step - loss: 0.8560 - acc: 0.6698 - val_loss: 0.9211 - val_acc: 0.6449\n",
            "Epoch 22/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8982 - acc: 0.6535\n",
            "Epoch 00022: val_acc did not improve from 0.72443\n",
            "46/46 [==============================] - 22s 474ms/step - loss: 0.8982 - acc: 0.6535 - val_loss: 0.9426 - val_acc: 0.6392\n",
            "Epoch 23/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8803 - acc: 0.6685\n",
            "Epoch 00023: val_acc did not improve from 0.72443\n",
            "46/46 [==============================] - 22s 479ms/step - loss: 0.8803 - acc: 0.6685 - val_loss: 0.7537 - val_acc: 0.6619\n",
            "Epoch 24/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8257 - acc: 0.6895\n",
            "Epoch 00024: val_acc did not improve from 0.72443\n",
            "46/46 [==============================] - 22s 475ms/step - loss: 0.8257 - acc: 0.6895 - val_loss: 0.8246 - val_acc: 0.6705\n",
            "Epoch 25/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8198 - acc: 0.6827\n",
            "Epoch 00025: val_acc did not improve from 0.72443\n",
            "46/46 [==============================] - 22s 478ms/step - loss: 0.8198 - acc: 0.6827 - val_loss: 0.8704 - val_acc: 0.6591\n",
            "Epoch 26/350\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8156 - acc: 0.6923"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-374187935c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m hist=model_1.fit_generator(train_generator,epochs=350,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n\u001b[0;32m----> 3\u001b[0;31m   validation_steps=len(validation_generator)//32,callbacks=[callbacks_list_2])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uSkXsjsi53X3",
        "outputId": "484ce144-2fff-412d-d009-7dedb277937b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_1.compile(optimizer='adam',metrics='acc',loss='categorical_crossentropy')\n",
        "hist=model_1.fit_generator(train_generator,epochs=200,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n",
        "  validation_steps=len(validation_generator)//32,callbacks=[callbacks_list_1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " 2/46 [>.............................] - ETA: 9s - loss: 0.3396 - acc: 0.9062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1606s vs `on_train_batch_end` time: 0.2675s). Check your callbacks.\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4647 - acc: 0.8247\n",
            "Epoch 00001: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 548ms/step - loss: 0.4647 - acc: 0.8247 - val_loss: 0.4942 - val_acc: 0.8267\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4219 - acc: 0.8417\n",
            "Epoch 00002: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 537ms/step - loss: 0.4219 - acc: 0.8417 - val_loss: 0.4554 - val_acc: 0.8551\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4042 - acc: 0.8485\n",
            "Epoch 00003: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.4042 - acc: 0.8485 - val_loss: 0.3722 - val_acc: 0.8608\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4060 - acc: 0.8512\n",
            "Epoch 00004: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.4060 - acc: 0.8512 - val_loss: 0.3916 - val_acc: 0.8381\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4337 - acc: 0.8431\n",
            "Epoch 00005: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 541ms/step - loss: 0.4337 - acc: 0.8431 - val_loss: 0.4150 - val_acc: 0.8466\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3962 - acc: 0.8560\n",
            "Epoch 00006: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3962 - acc: 0.8560 - val_loss: 0.3637 - val_acc: 0.8665\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4154 - acc: 0.8438\n",
            "Epoch 00007: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.4154 - acc: 0.8438 - val_loss: 0.5259 - val_acc: 0.8210\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4710 - acc: 0.8322\n",
            "Epoch 00008: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 537ms/step - loss: 0.4710 - acc: 0.8322 - val_loss: 0.5012 - val_acc: 0.8267\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4130 - acc: 0.8533\n",
            "Epoch 00009: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.4130 - acc: 0.8533 - val_loss: 0.4660 - val_acc: 0.8267\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4295 - acc: 0.8458\n",
            "Epoch 00010: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.4295 - acc: 0.8458 - val_loss: 0.4124 - val_acc: 0.8551\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4320 - acc: 0.8478\n",
            "Epoch 00011: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.4320 - acc: 0.8478 - val_loss: 0.4326 - val_acc: 0.8438\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4712 - acc: 0.8302\n",
            "Epoch 00012: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4712 - acc: 0.8302 - val_loss: 0.3881 - val_acc: 0.8438\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4557 - acc: 0.8383\n",
            "Epoch 00013: val_acc did not improve from 0.88021\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.4557 - acc: 0.8383 - val_loss: 0.4015 - val_acc: 0.8494\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4365 - acc: 0.8424\n",
            "Epoch 00014: val_acc improved from 0.88021 to 0.88068, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 25s 551ms/step - loss: 0.4365 - acc: 0.8424 - val_loss: 0.3818 - val_acc: 0.8807\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3807 - acc: 0.8614\n",
            "Epoch 00015: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3807 - acc: 0.8614 - val_loss: 0.4280 - val_acc: 0.8494\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3906 - acc: 0.8499\n",
            "Epoch 00016: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3906 - acc: 0.8499 - val_loss: 0.3320 - val_acc: 0.8807\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4392 - acc: 0.8417\n",
            "Epoch 00017: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 537ms/step - loss: 0.4392 - acc: 0.8417 - val_loss: 0.3461 - val_acc: 0.8608\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3930 - acc: 0.8499\n",
            "Epoch 00018: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 537ms/step - loss: 0.3930 - acc: 0.8499 - val_loss: 0.3668 - val_acc: 0.8722\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3952 - acc: 0.8635\n",
            "Epoch 00019: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3952 - acc: 0.8635 - val_loss: 0.4656 - val_acc: 0.8352\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4152 - acc: 0.8512\n",
            "Epoch 00020: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4152 - acc: 0.8512 - val_loss: 0.4011 - val_acc: 0.8438\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3762 - acc: 0.8635\n",
            "Epoch 00021: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3762 - acc: 0.8635 - val_loss: 0.3665 - val_acc: 0.8750\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.8512\n",
            "Epoch 00022: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3867 - acc: 0.8512 - val_loss: 0.3730 - val_acc: 0.8665\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4011 - acc: 0.8539\n",
            "Epoch 00023: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.4011 - acc: 0.8539 - val_loss: 0.4368 - val_acc: 0.8523\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3861 - acc: 0.8655\n",
            "Epoch 00024: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3861 - acc: 0.8655 - val_loss: 0.3548 - val_acc: 0.8778\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3536 - acc: 0.8702\n",
            "Epoch 00025: val_acc did not improve from 0.88068\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3536 - acc: 0.8702 - val_loss: 0.4437 - val_acc: 0.8466\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4038 - acc: 0.8553\n",
            "Epoch 00026: val_acc improved from 0.88068 to 0.88920, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 25s 553ms/step - loss: 0.4038 - acc: 0.8553 - val_loss: 0.3143 - val_acc: 0.8892\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3958 - acc: 0.8553\n",
            "Epoch 00027: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 25s 536ms/step - loss: 0.3958 - acc: 0.8553 - val_loss: 0.3335 - val_acc: 0.8778\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3751 - acc: 0.8723Epoch 29/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4096 - acc: 0.8492\n",
            "Epoch 00029: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4096 - acc: 0.8492 - val_loss: 0.3374 - val_acc: 0.8807\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3721 - acc: 0.8587\n",
            "Epoch 00030: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 25s 537ms/step - loss: 0.3721 - acc: 0.8587 - val_loss: 0.3432 - val_acc: 0.8864\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3273 - acc: 0.8777\n",
            "Epoch 00031: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3273 - acc: 0.8777 - val_loss: 0.3941 - val_acc: 0.8665\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3771 - acc: 0.8607\n",
            "Epoch 00032: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3771 - acc: 0.8607 - val_loss: 0.3916 - val_acc: 0.8665\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4010 - acc: 0.8505\n",
            "Epoch 00033: val_acc did not improve from 0.88920\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.4010 - acc: 0.8505 - val_loss: 0.3428 - val_acc: 0.8835\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4052 - acc: 0.8471\n",
            "Epoch 00034: val_acc improved from 0.88920 to 0.89489, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 26s 555ms/step - loss: 0.4052 - acc: 0.8471 - val_loss: 0.2966 - val_acc: 0.8949\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4331 - acc: 0.8315\n",
            "Epoch 00035: val_acc improved from 0.89489 to 0.90057, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 25s 553ms/step - loss: 0.4331 - acc: 0.8315 - val_loss: 0.3021 - val_acc: 0.9006\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3785 - acc: 0.8492\n",
            "Epoch 00036: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3785 - acc: 0.8492 - val_loss: 0.3948 - val_acc: 0.8438\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3937 - acc: 0.8655\n",
            "Epoch 00037: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3937 - acc: 0.8655 - val_loss: 0.3510 - val_acc: 0.8807\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3966 - acc: 0.8505\n",
            "Epoch 00038: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 533ms/step - loss: 0.3966 - acc: 0.8505 - val_loss: 0.3910 - val_acc: 0.8523\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3348 - acc: 0.8662\n",
            "Epoch 00039: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3348 - acc: 0.8662 - val_loss: 0.3597 - val_acc: 0.8750\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3814 - acc: 0.8635\n",
            "Epoch 00040: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3814 - acc: 0.8635 - val_loss: 0.3376 - val_acc: 0.8778\n",
            "Epoch 41/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4231 - acc: 0.8553\n",
            "Epoch 00041: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4231 - acc: 0.8553 - val_loss: 0.4265 - val_acc: 0.8352\n",
            "Epoch 42/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3883 - acc: 0.8607\n",
            "Epoch 00042: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3883 - acc: 0.8607 - val_loss: 0.3886 - val_acc: 0.8608\n",
            "Epoch 43/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3301 - acc: 0.8716\n",
            "Epoch 00043: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3301 - acc: 0.8716 - val_loss: 0.4167 - val_acc: 0.8551\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4286 - acc: 0.8499\n",
            "Epoch 00044: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.4286 - acc: 0.8499 - val_loss: 0.3701 - val_acc: 0.8523\n",
            "Epoch 45/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4002 - acc: 0.8546\n",
            "Epoch 00045: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4002 - acc: 0.8546 - val_loss: 0.4022 - val_acc: 0.8523\n",
            "Epoch 46/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3589 - acc: 0.8716\n",
            "Epoch 00046: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3589 - acc: 0.8716 - val_loss: 0.3730 - val_acc: 0.8580\n",
            "Epoch 47/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3703 - acc: 0.8662\n",
            "Epoch 00047: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 529ms/step - loss: 0.3703 - acc: 0.8662 - val_loss: 0.2801 - val_acc: 0.8920\n",
            "Epoch 48/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3744 - acc: 0.8594\n",
            "Epoch 00048: val_acc did not improve from 0.90057\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3744 - acc: 0.8594 - val_loss: 0.3512 - val_acc: 0.8835\n",
            "Epoch 49/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3713 - acc: 0.8696\n",
            "Epoch 00049: val_acc improved from 0.90057 to 0.91477, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 25s 549ms/step - loss: 0.3713 - acc: 0.8696 - val_loss: 0.2732 - val_acc: 0.9148\n",
            "Epoch 50/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3724 - acc: 0.8621\n",
            "Epoch 00050: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3724 - acc: 0.8621 - val_loss: 0.3901 - val_acc: 0.8466\n",
            "Epoch 51/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3365 - acc: 0.8764\n",
            "Epoch 00051: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3365 - acc: 0.8764 - val_loss: 0.3294 - val_acc: 0.8693\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3301 - acc: 0.8893\n",
            "Epoch 00052: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3301 - acc: 0.8893 - val_loss: 0.4005 - val_acc: 0.8352\n",
            "Epoch 53/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3406 - acc: 0.8811\n",
            "Epoch 00053: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3406 - acc: 0.8811 - val_loss: 0.3760 - val_acc: 0.8693\n",
            "Epoch 54/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3943 - acc: 0.8641\n",
            "Epoch 00054: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3943 - acc: 0.8641 - val_loss: 0.3221 - val_acc: 0.8750\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3843 - acc: 0.8655\n",
            "Epoch 00055: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3843 - acc: 0.8655 - val_loss: 0.3457 - val_acc: 0.8580\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3905 - acc: 0.8499\n",
            "Epoch 00056: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3905 - acc: 0.8499 - val_loss: 0.3336 - val_acc: 0.8750\n",
            "Epoch 57/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3298 - acc: 0.8859\n",
            "Epoch 00057: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3298 - acc: 0.8859 - val_loss: 0.3672 - val_acc: 0.8381\n",
            "Epoch 58/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3524 - acc: 0.8723\n",
            "Epoch 00058: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3524 - acc: 0.8723 - val_loss: 0.3026 - val_acc: 0.8693\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3685 - acc: 0.8621\n",
            "Epoch 00059: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3685 - acc: 0.8621 - val_loss: 0.3263 - val_acc: 0.8835\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3798 - acc: 0.8546\n",
            "Epoch 00060: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3798 - acc: 0.8546 - val_loss: 0.3848 - val_acc: 0.8750\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3771 - acc: 0.8764\n",
            "Epoch 00061: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3771 - acc: 0.8764 - val_loss: 0.3548 - val_acc: 0.8750\n",
            "Epoch 62/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3642 - acc: 0.8621\n",
            "Epoch 00062: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3642 - acc: 0.8621 - val_loss: 0.3613 - val_acc: 0.8665\n",
            "Epoch 63/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3384 - acc: 0.8798\n",
            "Epoch 00063: val_acc did not improve from 0.91477\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3384 - acc: 0.8798 - val_loss: 0.2981 - val_acc: 0.8892\n",
            "Epoch 64/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3650 - acc: 0.8560\n",
            "Epoch 00064: val_acc improved from 0.91477 to 0.92614, saving model to best_model_1.hdf5\n",
            "46/46 [==============================] - 25s 551ms/step - loss: 0.3650 - acc: 0.8560 - val_loss: 0.2500 - val_acc: 0.9261\n",
            "Epoch 65/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3811 - acc: 0.8641\n",
            "Epoch 00065: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 25s 535ms/step - loss: 0.3811 - acc: 0.8641 - val_loss: 0.3805 - val_acc: 0.8665\n",
            "Epoch 66/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3430 - acc: 0.8689\n",
            "Epoch 00066: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 25s 533ms/step - loss: 0.3430 - acc: 0.8689 - val_loss: 0.3494 - val_acc: 0.8778\n",
            "Epoch 67/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3514 - acc: 0.8743\n",
            "Epoch 00067: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3514 - acc: 0.8743 - val_loss: 0.2167 - val_acc: 0.9119\n",
            "Epoch 68/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3058 - acc: 0.8920\n",
            "Epoch 00068: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3058 - acc: 0.8920 - val_loss: 0.3423 - val_acc: 0.8693\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3458 - acc: 0.8736\n",
            "Epoch 00069: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3458 - acc: 0.8736 - val_loss: 0.3126 - val_acc: 0.8920\n",
            "Epoch 70/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3768 - acc: 0.8689\n",
            "Epoch 00070: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3768 - acc: 0.8689 - val_loss: 0.3380 - val_acc: 0.8977\n",
            "Epoch 71/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4215 - acc: 0.8444\n",
            "Epoch 00071: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 529ms/step - loss: 0.4215 - acc: 0.8444 - val_loss: 0.4094 - val_acc: 0.8409\n",
            "Epoch 72/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3537 - acc: 0.8757\n",
            "Epoch 00072: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3537 - acc: 0.8757 - val_loss: 0.3837 - val_acc: 0.8722\n",
            "Epoch 73/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3548 - acc: 0.8723\n",
            "Epoch 00073: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3548 - acc: 0.8723 - val_loss: 0.3319 - val_acc: 0.8778\n",
            "Epoch 74/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3297 - acc: 0.8736\n",
            "Epoch 00074: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 529ms/step - loss: 0.3297 - acc: 0.8736 - val_loss: 0.4186 - val_acc: 0.8409\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3708 - acc: 0.8601\n",
            "Epoch 00075: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3708 - acc: 0.8601 - val_loss: 0.4299 - val_acc: 0.8125\n",
            "Epoch 76/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3367 - acc: 0.8818\n",
            "Epoch 00076: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3367 - acc: 0.8818 - val_loss: 0.3546 - val_acc: 0.8778\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3528 - acc: 0.8621\n",
            "Epoch 00077: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3528 - acc: 0.8621 - val_loss: 0.3787 - val_acc: 0.8665\n",
            "Epoch 78/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3629 - acc: 0.8838\n",
            "Epoch 00078: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 532ms/step - loss: 0.3629 - acc: 0.8838 - val_loss: 0.3413 - val_acc: 0.8693\n",
            "Epoch 79/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3262 - acc: 0.8784\n",
            "Epoch 00079: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 25s 534ms/step - loss: 0.3262 - acc: 0.8784 - val_loss: 0.3374 - val_acc: 0.8693\n",
            "Epoch 80/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3356 - acc: 0.8791\n",
            "Epoch 00080: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 529ms/step - loss: 0.3356 - acc: 0.8791 - val_loss: 0.3330 - val_acc: 0.8551\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4027 - acc: 0.8641\n",
            "Epoch 00081: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.4027 - acc: 0.8641 - val_loss: 0.3955 - val_acc: 0.8381\n",
            "Epoch 82/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3414 - acc: 0.8702\n",
            "Epoch 00082: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 531ms/step - loss: 0.3414 - acc: 0.8702 - val_loss: 0.2476 - val_acc: 0.9176\n",
            "Epoch 83/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3455 - acc: 0.8675\n",
            "Epoch 00083: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3455 - acc: 0.8675 - val_loss: 0.2780 - val_acc: 0.8864\n",
            "Epoch 84/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3447 - acc: 0.8689\n",
            "Epoch 00084: val_acc did not improve from 0.92614\n",
            "46/46 [==============================] - 24s 530ms/step - loss: 0.3447 - acc: 0.8689 - val_loss: 0.3450 - val_acc: 0.8523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4wBRnlAU53YO",
        "outputId": "3d32e6ec-09ed-46fb-e9eb-96ce075199e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history['loss'],label='train loss')\n",
        "plt.plot(hist.history['val_loss'],label='val loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f073afc6940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hV-uVPVz53Yk",
        "outputId": "f47336b0-a2e7-49a7-da0f-cb9ab0efef28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(hist.history['acc'],label='train acc')\n",
        "plt.plot(hist.history['val_acc'],label='val acc')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f073afa84a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrLnTMNts0K",
        "outputId": "887e38e8-80c1-404d-9b25-88ef49738ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/best_model_2.h5')\n",
        "#model_2.load_weights('/content/best_model_2.h5') #loading weights of best model\n",
        "score= model.evaluate(validation_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 41s 110ms/step - loss: 0.2574 - accuracy: 0.9100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymN6x3rzdB8q",
        "outputId": "09c1c407-9382-4946-9d27-14cfb95f7940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#model_1.load_weights('/content/best_model_2.hdf5') #loading weights of best model\n",
        "model = load_model('/content/best_model_2.hdf5')\n",
        "score= model.evaluate(validation_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 137s 366ms/step - loss: 0.3966 - acc: 0.8551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IatJYJNSrAm"
      },
      "source": [
        "mkdir test_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFZO91-QiT2k"
      },
      "source": [
        "for i in os.listdir('/content/test/'):\n",
        "  if i!='test_images':\n",
        "    shutil.move('/content/test/'+i,'/content/test/test_image/')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kPtPySwZ53Y5",
        "outputId": "09a37873-25f3-4de3-9dac-c245a74dbd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "test_df=pd.read_csv('test.csv')\n",
        "test_df['id']=test_df['id'].apply(lambda x:str(x)+'.png')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60001.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60003.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60004.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60005.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id\n",
              "0  60001.png\n",
              "1  60002.png\n",
              "2  60003.png\n",
              "3  60004.png\n",
              "4  60005.png"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUHkeNW9g0Yr",
        "outputId": "8c764901-57cd-4af7-fa3b-7aa12505a07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1/255.) \n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=\"/content/test/\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBYPL5_YcJhh",
        "outputId": "d94efb2a-7e8c-4c72-b883-5a2242739c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "model = load_model('/content/best_model_2.h5')\n",
        "test_generator.reset()\n",
        "pred=model.predict_generator(test_generator,steps=STEP_SIZE_TEST,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 52s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BjsoirruERk",
        "outputId": "20eb0868-7f0c-46f2-d7f6-3e30cc83f8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "model = load_model('/content/best_model_2.hdf5')\n",
        "test_generator.reset()\n",
        "pred=model.predict_generator(test_generator,steps=STEP_SIZE_TEST,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    1/10000 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_predict_batch_end` time: 0.0112s). Check your callbacks.\n",
            "10000/10000 [==============================] - 82s 8ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzXH5X9bLJX",
        "outputId": "ee7417b4-8cd9-4ee4-c8ca-c2c052a99c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "test_pred=np.argmax(pred,axis=1)\n",
        "test_pred_df=test_df.copy()\n",
        "test_pred_df['label']=test_pred\n",
        "test_pred_df['id']=test_pred_df['id'].apply(lambda x:x.split('.')[0])\n",
        "print(test_pred_df.shape)\n",
        "test_pred_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60001</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60002</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60003</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60005</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60006</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>60007</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>60008</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>60009</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>60010</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  label\n",
              "0  60001      9\n",
              "1  60002      2\n",
              "2  60003      1\n",
              "3  60004      1\n",
              "4  60005      6\n",
              "5  60006      1\n",
              "6  60007      4\n",
              "7  60008      6\n",
              "8  60009      5\n",
              "9  60010      7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFyWB2xVgHyk"
      },
      "source": [
        "test_pred_df.to_csv('test_pred_df_9.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcHvKCfOU8je"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On-qVcVDU8Z6"
      },
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDZ90oNOVEEq"
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "compression = 0.5\n",
        "num_filter = 60\n",
        "num_classes = 10\n",
        "l = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdB08yYTVBnv"
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnDmIA1U-G_"
      },
      "source": [
        "input = layers.Input(shape=(224, 224, 3,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (5,5), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter)\n",
        "First_Transition = transition(First_Block, num_filter)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter)\n",
        "Second_Transition = transition(Second_Block, num_filter)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter)\n",
        "Third_Transition = transition(Third_Block, num_filter)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter)\n",
        "output = output_layer(Last_Block)\n",
        "model_2 = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVLtrPsbW3_x",
        "outputId": "9f4cc65b-d1c1-47bc-f3fa-d326fcf35c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "################ Tensorboard callback##########\n",
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "import datetime\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "if not os.path.exists('my_folder'):\n",
        "  os.makedirs('my_folder')\n",
        "filepath=\"best_model_2.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',save_best_only=True, mode='max',verbose=1)\n",
        "\n",
        "#https://towardsdatascience.com/neural-network-with-tensorflow-how-to-stop-training-using-callback-5c8d575c18a9\n",
        "from keras.callbacks import Callback\n",
        "TEST_ACCURACY_THRESHOLD = 0.999\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        if(logs.get('val_accuracy') > TEST_ACCURACY_THRESHOLD):   \n",
        "          print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(TEST_ACCURACY_THRESHOLD*100))   \n",
        "          self.model.stop_training = True\n",
        "early_stop = myCallback()\n",
        "\n",
        "final_callbacks=[checkpoint,early_stop,tensorboard_callback]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3tlwu6mW-ps",
        "outputId": "7a743bad-dca2-435e-e0bd-a0b5a6634740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist=model_2.fit_generator(train_generator,epochs=200,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n",
        "  validation_steps=len(validation_generator)//32,callbacks=[final_callbacks])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " 2/46 [>.............................] - ETA: 17s - loss: 5.2217 - accuracy: 0.0469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2170s vs `on_train_batch_end` time: 0.5879s). Check your callbacks.\n",
            "46/46 [==============================] - ETA: 0s - loss: 2.7469 - accuracy: 0.3682\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.10227, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 28s 598ms/step - loss: 2.7469 - accuracy: 0.3682 - val_loss: 3.2178 - val_accuracy: 0.1023\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.4779 - accuracy: 0.5204\n",
            "Epoch 00002: val_accuracy did not improve from 0.10227\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 1.4779 - accuracy: 0.5204 - val_loss: 4.1082 - val_accuracy: 0.0938\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.3518 - accuracy: 0.5496\n",
            "Epoch 00003: val_accuracy improved from 0.10227 to 0.11364, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 1.3518 - accuracy: 0.5496 - val_loss: 5.3896 - val_accuracy: 0.1136\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.3309 - accuracy: 0.5333\n",
            "Epoch 00004: val_accuracy did not improve from 0.11364\n",
            "46/46 [==============================] - 26s 575ms/step - loss: 1.3309 - accuracy: 0.5333 - val_loss: 7.7521 - val_accuracy: 0.0994\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1620 - accuracy: 0.5815\n",
            "Epoch 00005: val_accuracy did not improve from 0.11364\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 1.1620 - accuracy: 0.5815 - val_loss: 7.9939 - val_accuracy: 0.0881\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1427 - accuracy: 0.5897\n",
            "Epoch 00006: val_accuracy did not improve from 0.11364\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 1.1427 - accuracy: 0.5897 - val_loss: 6.6035 - val_accuracy: 0.1108\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1370 - accuracy: 0.6053\n",
            "Epoch 00007: val_accuracy did not improve from 0.11364\n",
            "46/46 [==============================] - 27s 584ms/step - loss: 1.1370 - accuracy: 0.6053 - val_loss: 6.5388 - val_accuracy: 0.0938\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.5938\n",
            "Epoch 00008: val_accuracy improved from 0.11364 to 0.20739, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 1.0988 - accuracy: 0.5938 - val_loss: 2.8606 - val_accuracy: 0.2074\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.0469 - accuracy: 0.6332\n",
            "Epoch 00009: val_accuracy did not improve from 0.20739\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 1.0469 - accuracy: 0.6332 - val_loss: 8.4427 - val_accuracy: 0.1392\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9857 - accuracy: 0.6617\n",
            "Epoch 00010: val_accuracy improved from 0.20739 to 0.53125, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 584ms/step - loss: 0.9857 - accuracy: 0.6617 - val_loss: 1.3655 - val_accuracy: 0.5312\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9579 - accuracy: 0.6495\n",
            "Epoch 00011: val_accuracy improved from 0.53125 to 0.60795, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.9579 - accuracy: 0.6495 - val_loss: 1.1717 - val_accuracy: 0.6080\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.6569\n",
            "Epoch 00012: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.9611 - accuracy: 0.6569 - val_loss: 4.2646 - val_accuracy: 0.1534\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.6610\n",
            "Epoch 00013: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.9335 - accuracy: 0.6610 - val_loss: 4.1217 - val_accuracy: 0.2017\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6726\n",
            "Epoch 00014: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.9017 - accuracy: 0.6726 - val_loss: 7.6593 - val_accuracy: 0.1648\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9077 - accuracy: 0.6712\n",
            "Epoch 00015: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.9077 - accuracy: 0.6712 - val_loss: 8.0176 - val_accuracy: 0.1761\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9060 - accuracy: 0.6766\n",
            "Epoch 00016: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.9060 - accuracy: 0.6766 - val_loss: 6.2570 - val_accuracy: 0.1989\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8560 - accuracy: 0.6875\n",
            "Epoch 00017: val_accuracy did not improve from 0.60795\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.8560 - accuracy: 0.6875 - val_loss: 1.5622 - val_accuracy: 0.4574\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.7038\n",
            "Epoch 00018: val_accuracy improved from 0.60795 to 0.69602, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 584ms/step - loss: 0.8462 - accuracy: 0.7038 - val_loss: 0.7895 - val_accuracy: 0.6960\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8038 - accuracy: 0.7024\n",
            "Epoch 00019: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.8038 - accuracy: 0.7024 - val_loss: 0.9025 - val_accuracy: 0.6932\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8304 - accuracy: 0.7045\n",
            "Epoch 00020: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 582ms/step - loss: 0.8304 - accuracy: 0.7045 - val_loss: 0.9541 - val_accuracy: 0.6420\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7830 - accuracy: 0.7065\n",
            "Epoch 00021: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.7830 - accuracy: 0.7065 - val_loss: 2.9369 - val_accuracy: 0.2699\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.8032 - accuracy: 0.6882\n",
            "Epoch 00022: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.8032 - accuracy: 0.6882 - val_loss: 1.5836 - val_accuracy: 0.4716\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7491 - accuracy: 0.7378\n",
            "Epoch 00023: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.7491 - accuracy: 0.7378 - val_loss: 1.1658 - val_accuracy: 0.6619\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7577 - accuracy: 0.7208\n",
            "Epoch 00024: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 26s 576ms/step - loss: 0.7577 - accuracy: 0.7208 - val_loss: 1.5379 - val_accuracy: 0.4403\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7344\n",
            "Epoch 00025: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.6967 - accuracy: 0.7344 - val_loss: 1.9635 - val_accuracy: 0.4176\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.7310\n",
            "Epoch 00026: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.7242 - accuracy: 0.7310 - val_loss: 0.8247 - val_accuracy: 0.6818\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.7147\n",
            "Epoch 00027: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.7700 - accuracy: 0.7147 - val_loss: 1.8109 - val_accuracy: 0.4176\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7387 - accuracy: 0.7262\n",
            "Epoch 00028: val_accuracy did not improve from 0.69602\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.7387 - accuracy: 0.7262 - val_loss: 3.2462 - val_accuracy: 0.3125\n",
            "Epoch 29/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.7357\n",
            "Epoch 00029: val_accuracy improved from 0.69602 to 0.70455, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.6969 - accuracy: 0.7357 - val_loss: 0.8707 - val_accuracy: 0.7045\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7117 - accuracy: 0.7432\n",
            "Epoch 00030: val_accuracy improved from 0.70455 to 0.78409, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 0.7117 - accuracy: 0.7432 - val_loss: 0.6030 - val_accuracy: 0.7841\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7103 - accuracy: 0.7357\n",
            "Epoch 00031: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.7103 - accuracy: 0.7357 - val_loss: 0.6842 - val_accuracy: 0.7500\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.7364\n",
            "Epoch 00032: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.6987 - accuracy: 0.7364 - val_loss: 3.7845 - val_accuracy: 0.3239\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.7432\n",
            "Epoch 00033: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 26s 576ms/step - loss: 0.6970 - accuracy: 0.7432 - val_loss: 0.6649 - val_accuracy: 0.7670\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.7588\n",
            "Epoch 00034: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.6740 - accuracy: 0.7588 - val_loss: 3.7467 - val_accuracy: 0.2756\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7425\n",
            "Epoch 00035: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 0.6874 - accuracy: 0.7425 - val_loss: 5.2953 - val_accuracy: 0.1903\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.7507\n",
            "Epoch 00036: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.6944 - accuracy: 0.7507 - val_loss: 1.3955 - val_accuracy: 0.5057\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.7357\n",
            "Epoch 00037: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.7110 - accuracy: 0.7357 - val_loss: 1.6673 - val_accuracy: 0.4119\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.7615\n",
            "Epoch 00038: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.6343 - accuracy: 0.7615 - val_loss: 2.3406 - val_accuracy: 0.3551\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7724\n",
            "Epoch 00039: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.6098 - accuracy: 0.7724 - val_loss: 1.2310 - val_accuracy: 0.5284\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.7731\n",
            "Epoch 00040: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 0.6148 - accuracy: 0.7731 - val_loss: 1.6432 - val_accuracy: 0.5028\n",
            "Epoch 41/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.7609\n",
            "Epoch 00041: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 582ms/step - loss: 0.6472 - accuracy: 0.7609 - val_loss: 0.6508 - val_accuracy: 0.7841\n",
            "Epoch 42/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6384 - accuracy: 0.7649\n",
            "Epoch 00042: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.6384 - accuracy: 0.7649 - val_loss: 1.0697 - val_accuracy: 0.6648\n",
            "Epoch 43/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.7643\n",
            "Epoch 00043: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 582ms/step - loss: 0.6112 - accuracy: 0.7643 - val_loss: 0.7363 - val_accuracy: 0.7330\n",
            "Epoch 44/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7799\n",
            "Epoch 00044: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.5990 - accuracy: 0.7799 - val_loss: 2.6796 - val_accuracy: 0.3807\n",
            "Epoch 45/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.7717\n",
            "Epoch 00045: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 26s 576ms/step - loss: 0.6270 - accuracy: 0.7717 - val_loss: 1.5648 - val_accuracy: 0.5142\n",
            "Epoch 46/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7867\n",
            "Epoch 00046: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 582ms/step - loss: 0.6079 - accuracy: 0.7867 - val_loss: 0.5792 - val_accuracy: 0.7670\n",
            "Epoch 47/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.7846\n",
            "Epoch 00047: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 0.5914 - accuracy: 0.7846 - val_loss: 2.2746 - val_accuracy: 0.3466\n",
            "Epoch 48/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7779\n",
            "Epoch 00048: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 583ms/step - loss: 0.6199 - accuracy: 0.7779 - val_loss: 5.0601 - val_accuracy: 0.2244\n",
            "Epoch 49/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.8016\n",
            "Epoch 00049: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 583ms/step - loss: 0.5586 - accuracy: 0.8016 - val_loss: 0.7175 - val_accuracy: 0.7415\n",
            "Epoch 50/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.7690\n",
            "Epoch 00050: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.6252 - accuracy: 0.7690 - val_loss: 1.1773 - val_accuracy: 0.5881\n",
            "Epoch 51/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.7982\n",
            "Epoch 00051: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.5806 - accuracy: 0.7982 - val_loss: 1.3899 - val_accuracy: 0.5625\n",
            "Epoch 52/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7806\n",
            "Epoch 00052: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.5858 - accuracy: 0.7806 - val_loss: 0.7328 - val_accuracy: 0.7159\n",
            "Epoch 53/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.8105\n",
            "Epoch 00053: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5406 - accuracy: 0.8105 - val_loss: 0.5760 - val_accuracy: 0.7841\n",
            "Epoch 54/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7908\n",
            "Epoch 00054: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.5607 - accuracy: 0.7908 - val_loss: 0.7081 - val_accuracy: 0.7557\n",
            "Epoch 55/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.7969\n",
            "Epoch 00055: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5381 - accuracy: 0.7969 - val_loss: 0.8238 - val_accuracy: 0.7301\n",
            "Epoch 56/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5788 - accuracy: 0.7989\n",
            "Epoch 00056: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 26s 575ms/step - loss: 0.5788 - accuracy: 0.7989 - val_loss: 0.6199 - val_accuracy: 0.7812\n",
            "Epoch 57/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.7935\n",
            "Epoch 00057: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 579ms/step - loss: 0.5797 - accuracy: 0.7935 - val_loss: 1.2155 - val_accuracy: 0.5795\n",
            "Epoch 58/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.8132\n",
            "Epoch 00058: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.5262 - accuracy: 0.8132 - val_loss: 0.6610 - val_accuracy: 0.7642\n",
            "Epoch 59/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7908\n",
            "Epoch 00059: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.5719 - accuracy: 0.7908 - val_loss: 1.0809 - val_accuracy: 0.6051\n",
            "Epoch 60/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8010\n",
            "Epoch 00060: val_accuracy did not improve from 0.78409\n",
            "46/46 [==============================] - 27s 580ms/step - loss: 0.5542 - accuracy: 0.8010 - val_loss: 0.7399 - val_accuracy: 0.7330\n",
            "Epoch 61/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.8003\n",
            "Epoch 00061: val_accuracy improved from 0.78409 to 0.79545, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.5487 - accuracy: 0.8003 - val_loss: 0.5522 - val_accuracy: 0.7955\n",
            "Epoch 62/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.7955\n",
            "Epoch 00062: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5641 - accuracy: 0.7955 - val_loss: 1.4375 - val_accuracy: 0.5028\n",
            "Epoch 63/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.8098\n",
            "Epoch 00063: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5215 - accuracy: 0.8098 - val_loss: 2.4732 - val_accuracy: 0.3409\n",
            "Epoch 64/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.8043\n",
            "Epoch 00064: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.5262 - accuracy: 0.8043 - val_loss: 0.7215 - val_accuracy: 0.7670\n",
            "Epoch 65/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.8030\n",
            "Epoch 00065: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 577ms/step - loss: 0.5410 - accuracy: 0.8030 - val_loss: 0.6322 - val_accuracy: 0.7812\n",
            "Epoch 66/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8173\n",
            "Epoch 00066: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5416 - accuracy: 0.8173 - val_loss: 2.1066 - val_accuracy: 0.4915\n",
            "Epoch 67/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.7894\n",
            "Epoch 00067: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 581ms/step - loss: 0.5959 - accuracy: 0.7894 - val_loss: 1.5344 - val_accuracy: 0.5767\n",
            "Epoch 68/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7928\n",
            "Epoch 00068: val_accuracy did not improve from 0.79545\n",
            "46/46 [==============================] - 27s 578ms/step - loss: 0.5645 - accuracy: 0.7928 - val_loss: 0.6278 - val_accuracy: 0.7955\n",
            "Epoch 69/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.8050\n",
            "Epoch 00069: val_accuracy improved from 0.79545 to 0.80114, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 27s 592ms/step - loss: 0.5666 - accuracy: 0.8050 - val_loss: 0.6132 - val_accuracy: 0.8011\n",
            "Epoch 70/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8010\n",
            "Epoch 00070: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 28s 605ms/step - loss: 0.5563 - accuracy: 0.8010 - val_loss: 0.6365 - val_accuracy: 0.7727\n",
            "Epoch 71/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.8064\n",
            "Epoch 00071: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 28s 600ms/step - loss: 0.5418 - accuracy: 0.8064 - val_loss: 1.0057 - val_accuracy: 0.6335\n",
            "Epoch 72/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.8077\n",
            "Epoch 00072: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 592ms/step - loss: 0.5085 - accuracy: 0.8077 - val_loss: 0.7495 - val_accuracy: 0.7500\n",
            "Epoch 73/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.7948\n",
            "Epoch 00073: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 594ms/step - loss: 0.5416 - accuracy: 0.7948 - val_loss: 1.4806 - val_accuracy: 0.5085\n",
            "Epoch 74/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.8132\n",
            "Epoch 00074: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 595ms/step - loss: 0.5248 - accuracy: 0.8132 - val_loss: 0.7328 - val_accuracy: 0.7614\n",
            "Epoch 75/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.8145\n",
            "Epoch 00075: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 28s 598ms/step - loss: 0.5455 - accuracy: 0.8145 - val_loss: 4.2382 - val_accuracy: 0.3807\n",
            "Epoch 76/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.8071\n",
            "Epoch 00076: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 593ms/step - loss: 0.5444 - accuracy: 0.8071 - val_loss: 1.5599 - val_accuracy: 0.5085\n",
            "Epoch 77/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.8111\n",
            "Epoch 00077: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.5096 - accuracy: 0.8111 - val_loss: 1.9140 - val_accuracy: 0.4290\n",
            "Epoch 78/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5452 - accuracy: 0.8057\n",
            "Epoch 00078: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 595ms/step - loss: 0.5452 - accuracy: 0.8057 - val_loss: 1.1539 - val_accuracy: 0.5540\n",
            "Epoch 79/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4963 - accuracy: 0.8397\n",
            "Epoch 00079: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 592ms/step - loss: 0.4963 - accuracy: 0.8397 - val_loss: 1.0163 - val_accuracy: 0.6648\n",
            "Epoch 80/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.8152\n",
            "Epoch 00080: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 589ms/step - loss: 0.5161 - accuracy: 0.8152 - val_loss: 0.8533 - val_accuracy: 0.6847\n",
            "Epoch 81/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.8288\n",
            "Epoch 00081: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 596ms/step - loss: 0.4989 - accuracy: 0.8288 - val_loss: 0.9952 - val_accuracy: 0.6392\n",
            "Epoch 82/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.8050\n",
            "Epoch 00082: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 595ms/step - loss: 0.5479 - accuracy: 0.8050 - val_loss: 0.6752 - val_accuracy: 0.7869\n",
            "Epoch 83/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.8288\n",
            "Epoch 00083: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 594ms/step - loss: 0.4860 - accuracy: 0.8288 - val_loss: 0.6788 - val_accuracy: 0.7244\n",
            "Epoch 84/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.8336\n",
            "Epoch 00084: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 595ms/step - loss: 0.4828 - accuracy: 0.8336 - val_loss: 1.4705 - val_accuracy: 0.5625\n",
            "Epoch 85/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.8213\n",
            "Epoch 00085: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.5162 - accuracy: 0.8213 - val_loss: 1.2116 - val_accuracy: 0.6051\n",
            "Epoch 86/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8268\n",
            "Epoch 00086: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 593ms/step - loss: 0.4715 - accuracy: 0.8268 - val_loss: 1.1460 - val_accuracy: 0.5824\n",
            "Epoch 87/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.8302\n",
            "Epoch 00087: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 592ms/step - loss: 0.5058 - accuracy: 0.8302 - val_loss: 1.3425 - val_accuracy: 0.5284\n",
            "Epoch 88/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.8152\n",
            "Epoch 00088: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.5075 - accuracy: 0.8152 - val_loss: 0.7906 - val_accuracy: 0.7415\n",
            "Epoch 89/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4943 - accuracy: 0.8288\n",
            "Epoch 00089: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.4943 - accuracy: 0.8288 - val_loss: 1.5399 - val_accuracy: 0.4886\n",
            "Epoch 90/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.8207\n",
            "Epoch 00090: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.5342 - accuracy: 0.8207 - val_loss: 1.1580 - val_accuracy: 0.5682\n",
            "Epoch 91/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.8247\n",
            "Epoch 00091: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 591ms/step - loss: 0.5050 - accuracy: 0.8247 - val_loss: 1.2728 - val_accuracy: 0.5511\n",
            "Epoch 92/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.8302\n",
            "Epoch 00092: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.4934 - accuracy: 0.8302 - val_loss: 0.7039 - val_accuracy: 0.7670\n",
            "Epoch 93/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.8220\n",
            "Epoch 00093: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 594ms/step - loss: 0.4863 - accuracy: 0.8220 - val_loss: 2.1863 - val_accuracy: 0.4943\n",
            "Epoch 94/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8573\n",
            "Epoch 00094: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.4245 - accuracy: 0.8573 - val_loss: 0.6963 - val_accuracy: 0.7301\n",
            "Epoch 95/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.8390\n",
            "Epoch 00095: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.4682 - accuracy: 0.8390 - val_loss: 1.1663 - val_accuracy: 0.5767\n",
            "Epoch 96/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.8302\n",
            "Epoch 00096: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 589ms/step - loss: 0.4643 - accuracy: 0.8302 - val_loss: 1.0343 - val_accuracy: 0.6136\n",
            "Epoch 97/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.8071\n",
            "Epoch 00097: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.5105 - accuracy: 0.8071 - val_loss: 1.0476 - val_accuracy: 0.5994\n",
            "Epoch 98/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.8315\n",
            "Epoch 00098: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.4620 - accuracy: 0.8315 - val_loss: 2.4165 - val_accuracy: 0.5114\n",
            "Epoch 99/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.8315\n",
            "Epoch 00099: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.4696 - accuracy: 0.8315 - val_loss: 1.0363 - val_accuracy: 0.6420\n",
            "Epoch 100/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.8349\n",
            "Epoch 00100: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.4675 - accuracy: 0.8349 - val_loss: 1.3593 - val_accuracy: 0.5682\n",
            "Epoch 101/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4450 - accuracy: 0.8478\n",
            "Epoch 00101: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 587ms/step - loss: 0.4450 - accuracy: 0.8478 - val_loss: 1.0148 - val_accuracy: 0.6705\n",
            "Epoch 102/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.8349\n",
            "Epoch 00102: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 589ms/step - loss: 0.4639 - accuracy: 0.8349 - val_loss: 1.4489 - val_accuracy: 0.5938\n",
            "Epoch 103/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.8247\n",
            "Epoch 00103: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 590ms/step - loss: 0.4922 - accuracy: 0.8247 - val_loss: 1.8909 - val_accuracy: 0.4517\n",
            "Epoch 104/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.8193\n",
            "Epoch 00104: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 594ms/step - loss: 0.4921 - accuracy: 0.8193 - val_loss: 1.9167 - val_accuracy: 0.4972\n",
            "Epoch 105/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.8173\n",
            "Epoch 00105: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.5230 - accuracy: 0.8173 - val_loss: 1.4481 - val_accuracy: 0.4886\n",
            "Epoch 106/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.8417\n",
            "Epoch 00106: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.4620 - accuracy: 0.8417 - val_loss: 1.2323 - val_accuracy: 0.6278\n",
            "Epoch 107/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8240\n",
            "Epoch 00107: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 588ms/step - loss: 0.4660 - accuracy: 0.8240 - val_loss: 2.6703 - val_accuracy: 0.4801\n",
            "Epoch 108/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.8349\n",
            "Epoch 00108: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.4690 - accuracy: 0.8349 - val_loss: 1.5603 - val_accuracy: 0.5369\n",
            "Epoch 109/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.8349\n",
            "Epoch 00109: val_accuracy did not improve from 0.80114\n",
            "46/46 [==============================] - 27s 585ms/step - loss: 0.4507 - accuracy: 0.8349 - val_loss: 1.6766 - val_accuracy: 0.5057\n",
            "Epoch 110/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.8539\n",
            "Epoch 00110: val_accuracy improved from 0.80114 to 0.82102, saving model to best_model_2.h5\n",
            "46/46 [==============================] - 28s 608ms/step - loss: 0.4231 - accuracy: 0.8539 - val_loss: 0.5480 - val_accuracy: 0.8210\n",
            "Epoch 111/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.8417\n",
            "Epoch 00111: val_accuracy did not improve from 0.82102\n",
            "46/46 [==============================] - 27s 584ms/step - loss: 0.4610 - accuracy: 0.8417 - val_loss: 0.8246 - val_accuracy: 0.7017\n",
            "Epoch 112/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.8261\n",
            "Epoch 00112: val_accuracy did not improve from 0.82102\n",
            "46/46 [==============================] - 27s 586ms/step - loss: 0.4561 - accuracy: 0.8261 - val_loss: 0.6352 - val_accuracy: 0.7670\n",
            "Epoch 113/200\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8349\n",
            "Epoch 00113: val_accuracy did not improve from 0.82102\n",
            "46/46 [==============================] - 27s 589ms/step - loss: 0.4326 - accuracy: 0.8349 - val_loss: 1.1267 - val_accuracy: 0.6818\n",
            "Epoch 114/200\n",
            "13/46 [=======>......................] - ETA: 15s - loss: 0.4680 - accuracy: 0.8173"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-c791ea37a6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m hist=model_2.fit_generator(train_generator,epochs=200,validation_data=(validation_generator),steps_per_epoch=len(train_generator)//32,\n\u001b[0;32m----> 2\u001b[0;31m   validation_steps=len(validation_generator)//32,callbacks=[final_callbacks])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}